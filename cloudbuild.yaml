steps:
  # Optionally restore build artifacts cached in Cloud Storage
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'restore-cache'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -euo pipefail
        CACHE_BUCKET="${_CACHE_BUCKET:-${PROJECT_ID}_cloudbuild}"
        CACHE_OBJECT="${_CACHE_OBJECT:-build-cache/backend-cache.tar.gz}"
        if gcloud storage cp "gs://${CACHE_BUCKET}/${CACHE_OBJECT}" cache.tar.gz; then
          echo "Restored cache archive gs://${CACHE_BUCKET}/${CACHE_OBJECT}"
          tar -xzf cache.tar.gz
        else
          echo "No cache archive found at gs://${CACHE_BUCKET}/${CACHE_OBJECT}; continuing without it."
        fi

  # Download Python wheels so Docker build can reuse them offline
  - name: 'python:3.11-slim'
    id: 'prepare-wheel-cache'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -euo pipefail
        mkdir -p build_cache/pip_wheels
        pip install --no-cache-dir --upgrade pip setuptools wheel
        pip download --no-deps -r requirements.txt -d build_cache/pip_wheels || echo "Wheel download completed (missing wheels may be built in Docker stage)."

  # Pull the previously built image to prime the Docker layer cache
  - name: 'gcr.io/cloud-builders/docker'
    id: 'pull-cache-image'
    entrypoint: 'bash'
    args:
      - '-c'
      - 'docker pull ${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:latest || exit 0'

  # Build the Docker image using cache-from to reuse layers
  - name: 'gcr.io/cloud-builders/docker'
    id: 'build-image'
    env:
      - 'DOCKER_BUILDKIT=1'
    args:
      - 'build'
      - '-t'
      - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:$COMMIT_SHA'
      - '-t'
      - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:latest'
      - '--cache-from'
      - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:latest'
      - '.'

  # Push the Docker image tags to Google Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    id: 'push-commit-image'
    args:
      - 'push'
      - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:$COMMIT_SHA'

  - name: 'gcr.io/cloud-builders/docker'
    id: 'push-latest-image'
    args:
      - 'push'
      - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:latest'

  # Optionally archive build artifacts back to Cloud Storage for future builds
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'save-cache'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -euo pipefail
        CACHE_BUCKET="${_CACHE_BUCKET:-${PROJECT_ID}_cloudbuild}"
        CACHE_OBJECT="${_CACHE_OBJECT:-build-cache/backend-cache.tar.gz}"
        CACHE_PATHS="${_CACHE_PATHS:-build_cache/pip_wheels}"
        IFS=',' read -ra DIRS <<< "${CACHE_PATHS}"
        INCLUDE_DIRS=()
        for DIR in "${DIRS[@]}"; do
          if [ -e "$DIR" ]; then
            INCLUDE_DIRS+=("$DIR")
          fi
        done
        if [ ${#INCLUDE_DIRS[@]} -gt 0 ]; then
          tar -czf cache.tar.gz "${INCLUDE_DIRS[@]}"
          gcloud storage cp cache.tar.gz "gs://${CACHE_BUCKET}/${CACHE_OBJECT}"
          echo "Uploaded cache archive to gs://${CACHE_BUCKET}/${CACHE_OBJECT}"
        else
          echo "No cache directories found; skipping upload."
        fi

  # Deploy to Cloud Run with secrets from Secret Manager
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - 'run'
      - 'deploy'
      - 'deeptutor'
      - '--image'
      - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:$COMMIT_SHA'
      - '--platform'
      - 'managed'
      - '--region'
      - 'us-central1'
      - '--allow-unauthenticated'
      - '--port'
      - '8001'
      - '--set-secrets'
      - 'GEMINI_API_KEY=gemini-api-key:latest'
      - '--set-secrets'
      - 'OPENAI_API_KEY=openai-api-key:latest'
      - '--set-secrets'
      - 'VERCEL_API_KEY=vercel-api-key:latest'
      - '--set-secrets'
      - 'VERTEX_API_KEY=vertex-api-key:latest'
      - '--set-secrets'
      - 'GITHUB_OAUTH_TOKEN=scrrlt-github-oauthtoken-2238c3:latest'
      - '--set-env-vars'
      - 'GOOGLE_CLOUD_PROJECT=$PROJECT_ID'
      - '--set-env-vars'
      - 'ENVIRONMENT=production'
      - '--set-env-vars'
      - 'LLM_PROVIDER=gemini'
      - '--set-env-vars'
      - 'LLM_MODEL=gemini-1.5-flash'
      - '--set-env-vars'
      - 'EMBEDDING_PROVIDER=openai'
      - '--set-env-vars'
      - 'EMBEDDING_MODEL=text-embedding-3-small'
      - '--set-env-vars'
      - 'EMBEDDING_DIMENSION=1536'
      - '--set-env-vars'
      - 'SECRET_KEY=deeptutor-production-secret-key'
      - '--set-env-vars'
      - 'CORS_ORIGINS=https://deeptutor.vercel.app,https://deeptutor-git-main-scrrlt.vercel.app'
      - '--gpu'
      - '8'
      - '--gpu-type'
      - 'nvidia-tesla-t4'
      - '--set-env-vars'
      - 'LOG_LEVEL=INFO'

# Store the image in Artifact Registry
images:
  - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:$COMMIT_SHA'
  - '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}:latest'

# Configure logging for service account usage
options:
  logging: CLOUD_LOGGING_ONLY
  defaultLogsBucketBehavior: REGIONAL_USER_OWNED_BUCKET
  machineType: 'E2_HIGHCPU_8'

# Set timeout
timeout: '1200s'

# Default substitution values (can be overridden via gcloud builds submit --substitutions)
substitutions:
  _LOCATION: 'us-central1'
  _REPOSITORY: 'deeptutor'
  _IMAGE: 'backend'
  _CACHE_BUCKET: ''
  _CACHE_OBJECT: ''
  _CACHE_PATHS: ''
