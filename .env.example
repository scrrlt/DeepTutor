﻿﻿# ==============================================================================
# DeepTutor Environment Configuration
# =========================================================================
# Copy this file to `.env` and fill in the values.
# Required fields are marked with [Required], optional fields with [Optional].

# ==============================================================================
# Pytest Configuration
# ========================================================================

# [Optional] Set to 1 to run LLM network tests. Requires LLM keys to be populated.
# If not set or set to 0, LLM network tests will be skipped.

# SSL inspection must be disabled to avoid SSL/TLS errors during tests.
# Set to 'true' to disable SSL verification for LLM network tests.
# WARNING: Only use this for testing. Never disable SSL verification in production.
DISABLE_SSL_VERIFY=true
# [Optional] For VS Code users: To enable the Python extension to load environment variables from this .env file, set `python.terminal.useEnvFile` to 'True' in your VS Code settings (settings.json). This setting ensures the environment is correctly configured for network tests. Note that this setting is for the VS Code terminal, not for the Python interpreter itself. It is recommended to add this setting to your workspace settings (`.vscode/settings.json`) or user settings.
# If unable to set via IDE, you can manually load the .env file in your test setup code as follows (PowerShell example):
#
# $envContent = Get-Content .env -Raw
# foreach ($line in ($envContent -split "`n")) {
#     if ($line -match '^([^=]+)=(.*)# }

# If you need to test specific providers, use LLM_BINDING=<provider> with the integration tests.

# =====================================
# Server Ports
# ==============================================================================

# [Optional] Backend API server port
BACKEND_PORT=8001

# [Optional] Frontend server port
FRONTEND_PORT=3782

# ==============================================================================
# LLM Configuration (Large Language Model)
# ==============================================================================
# Primary LLM for all AI operations (chat, research, solve, etc.)

# [Required] Provider binding: openai, azure_openai, anthropic,
# deepseek, openrouter, groq, together, mistral
# ollama, lm_studio, vllm, llama_cpp
LLM_BINDING=openai

# [Required] Model name (e.g., gpt-4o, deepseek-chat, claude-3-5-sonnet)
LLM_MODEL=gpt-4o

# [Required] API key for the LLM provider
LLM_API_KEY=sk-xxx

# [Required] API endpoint URL
LLM_HOST=https://api.openai.com/v1

# [Optional] API version (required for Azure OpenAI)
LLM_API_VERSION=

# ==============================================================================
# Embedding Configuration
# ==============================================================================
# Embedding model for RAG (Retrieval-Augmented Generation)

# [Required] Provider: openai, azure_openai, jina,
# cohere, huggingface, google, ollama, lm_studio
EMBEDDING_BINDING=openai

# [Required] Model name
EMBEDDING_MODEL=text-embedding-3-small

# [Required] API key
EMBEDDING_API_KEY=sk-xxx

# [Required] API endpoint URL
EMBEDDING_HOST=https://api.openai.com/v1

# [Required] Vector dimensions (must match model output)
EMBEDDING_DIMENSION=3072

# [Optional] API version (for Azure OpenAI)
EMBEDDING_API_VERSION=

# ==============================================================================
# TTS Configuration (Text-to-Speech)
# ==============================================================================
# Optional: Enable audio narration features

# [Optional] Provider: openai, azure_openai
TTS_BINDING=openai

# [Optional] TTS model name
TTS_MODEL=tts-1

# [Optional] API key (can be same as LLM_API_KEY for OpenAI)
TTS_API_KEY=sk-xxx

# [Optional] API endpoint URL
TTS_URL=https://api.openai.com/v1

# [Optional] Voice: alloy, echo, fable, onyx, nova, shimmer
TTS_VOICE=alloy

# [Optional] API version (for Azure OpenAI)
TTS_BINDING_API_VERSION=

# ==============================================================================
# Search Configuration (Web Search)
# ==============================================================================
# Optional: Enable web search capabilities

# [Optional] Provider: perplexity, tavily, serper, jina, exa
SEARCH_PROVIDER=perplexity

# [Optional] API key for your chosen search provider
SEARCH_API_KEY=pplx-xxx

# ==============================================================================
# Cloud Deployment Configuration
# ==============================================================================
# Required when deploying to cloud/remote servers

# [Optional] External API base URL for cloud deployment
# Set this to your server's public URL when deploying remotely
# Example: https://your-server.com:8001 or https://api.yourdomain.com
NEXT_PUBLIC_API_BASE_EXTERNAL=

# [Optional] Direct API base URL (alternative to above)
NEXT_PUBLIC_API_BASE=

# ==============================================================================
# Local LLM Configuration (for Ollama, LM Studio, etc.)
# ==============================================================================

# [Optional] Template mode for chat formatting: auto, chatml, llama3, gemma, etc.
# 'auto' attempts to detect from model name, 'none' disables template formatting
LOCAL_LLM_TEMPLATE_MODE=auto

# [Optional] Custom chat template (Jinja2 format) - overrides template mode
# Example: "{{ bos_token }}{% for message in messages %}..."
LOCAL_LLM_CHAT_TEMPLATE=

# ==============================================================================
# Debug & Development
# ==============================================================================

# [Optional] Disable SSL verification (not recommended for production)
# Accepts: 'true', '1', 'yes' (case-insensitive)
DISABLE_SSL_VERIFY=false

# ==============================================================================
# DashScope Configuration (Qwen via OpenAI-compatible API)
# ==============================================================================
# Set the HTTP base URL for DashScope regional endpoints. Common options:
# - US:  https://dashscope-us.aliyuncs.com/compatible-mode/v1
# - SG:  https://dashscope-intl.aliyuncs.com/compatible-mode/v1
# - CN:  https://dashscope.aliyuncs.com/compatible-mode/v1
# Full chat completions endpoint example:
#   https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions

DASHSCOPE_API_KEY=
DASHSCOPE_HTTP_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1
) {
#         $key = $matches[1].Trim()
#         $value = $matches[2].Trim()
#         [Environment]::SetEnvironmentVariable($key, $value, "Process")
#     }
# }# }

# If you need to test specific providers, use LLM_BINDING=<provider> with the integration tests.

# =====================================
# Server Ports
# ==============================================================================

# [OPTIONAL] Backend API server port (default: 8001)
# Change if port 8001 is already in use on your system
BACKEND_PORT=8001

# [OPTIONAL] Frontend server port (default: 3782)
# Change if port 3782 is already in use on your system
FRONTEND_PORT=3782

# ==============================================================================
# LLM CONFIGURATION (Large Language Model)
# ==============================================================================
# Primary LLM for all AI operations (chat, research, solve, code generation, etc.)
#
# SUPPORTED PROVIDERS:
# Cloud APIs: openai, anthropic, deepseek, groq, mistral, openrouter, together
# Local: ollama, lm_studio, vllm, llama_cpp
# Azure: azure_openai

# [REQUIRED] LLM provider binding
# Examples:
#   openai     - OpenAI API (GPT models)
#   anthropic  - Anthropic API (Claude models)
#   deepseek   - DeepSeek API (deepseek-chat, deepseek-reasoner)
#   groq       - Groq API (fast inference)
#   mistral    - Mistral AI API
#   openrouter - OpenRouter (access to many models)
#   together   - Together AI API
#   ollama     - Local Ollama server
#   lm_studio  - Local LM Studio
#   vllm       - Local vLLM server
#   llama_cpp  - Local llama.cpp server
#   azure_openai - Azure OpenAI service
LLM_BINDING=openai

# [REQUIRED] Model name - depends on your chosen provider
# Examples by provider:
# OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-5-sonnet-20241022, claude-3-haiku-20240307
# DeepSeek: deepseek-chat, deepseek-reasoner
# Groq: llama-3.2-90b-text-preview, mixtral-8x7b-32768
# Ollama: llama3.2:3b, codellama:7b, mistral:7b
# LM Studio: any model loaded in LM Studio
LLM_MODEL=gpt-4o

# [REQUIRED] API key for your LLM provider
# Get from: https://platform.openai.com/api-keys (OpenAI)
#          https://console.anthropic.com/ (Anthropic)
#          https://platform.deepseek.com/ (DeepSeek)
#          https://console.groq.com/ (Groq)
#          Not needed for local providers (ollama, lm_studio, etc.)
LLM_API_KEY=sk-your-openai-api-key-here

# [REQUIRED] API endpoint URL
# Examples:
# OpenAI: https://api.openai.com/v1
# Anthropic: https://api.anthropic.com/v1
# DeepSeek: https://api.deepseek.com
# Groq: https://api.groq.com/openai/v1
# Ollama: http://localhost:11434/v1
# LM Studio: http://localhost:1234/v1
# vLLM: http://localhost:8000/v1
# llama.cpp: http://localhost:8080/v1
# Azure: https://your-resource.openai.azure.com/
LLM_HOST=https://api.openai.com/v1

# [OPTIONAL] API version (required for Azure OpenAI)
# Examples: 2024-02-15-preview, 2023-12-01-preview
# Leave empty for non-Azure providers
LLM_API_VERSION=

# ==============================================================================
# LLM RETRY CONFIGURATION
# ==============================================================================
# Configure automatic retry behavior for LLM API calls

# [OPTIONAL] Maximum number of retry attempts (default: 3)
# Higher values increase reliability but may slow down responses
LLM_RETRY__MAX_RETRIES=3

# [OPTIONAL] Base delay between retries in seconds (default: 1.0)
# Used for exponential backoff calculations
LLM_RETRY__BASE_DELAY=1.0

# [OPTIONAL] Use exponential backoff for retries (default: true)
# When true: delays increase as 1s, 2s, 4s, 8s, etc.
# When false: fixed delay between all retries
LLM_RETRY__EXPONENTIAL_BACKOFF=true

# ==============================================================================
# EMBEDDING CONFIGURATION
# ==============================================================================
# Vector embeddings for RAG (Retrieval-Augmented Generation)
# Used to create searchable knowledge bases from documents
#
# SUPPORTED PROVIDERS: openai, azure_openai, cohere, jina, huggingface

# [REQUIRED] Embedding provider
# Examples: openai, azure_openai, cohere, jina
EMBEDDING_BINDING=openai

# [REQUIRED] Embedding model name
# Examples:
# OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# Cohere: embed-english-v3.0, embed-multilingual-v3.0
# Jina: jina-embeddings-v2-base-en, jina-embeddings-v2-small-en
EMBEDDING_MODEL=text-embedding-3-small

# [REQUIRED] API key for embedding provider
# Can be the same as LLM_API_KEY for OpenAI
EMBEDDING_API_KEY=sk-your-openai-api-key-here

# [REQUIRED] API endpoint URL for embeddings
# Usually the same as your LLM provider's base URL
EMBEDDING_HOST=https://api.openai.com/v1

# [REQUIRED] Vector dimensions (must match your chosen model)
# Examples:
# text-embedding-3-small: 1536
# text-embedding-3-large: 3072
# text-embedding-ada-002: 1536
EMBEDDING_DIMENSION=1536

# [OPTIONAL] API version (for Azure OpenAI embeddings)
EMBEDDING_API_VERSION=

# ==============================================================================
# TTS CONFIGURATION (Text-to-Speech)
# ==============================================================================
# Optional: Enable audio narration and voice features
#
# SUPPORTED PROVIDERS: openai, azure_openai

# [OPTIONAL] TTS provider (leave empty to disable TTS)
TTS_BINDING=openai

# [OPTIONAL] TTS model name
# OpenAI: tts-1, tts-1-hd
TTS_MODEL=tts-1

# [OPTIONAL] API key for TTS (can reuse LLM_API_KEY for OpenAI)
TTS_API_KEY=sk-your-openai-api-key-here

# [OPTIONAL] TTS API endpoint URL
TTS_HOST=https://api.openai.com/v1

# [OPTIONAL] Voice selection
# OpenAI voices: alloy, echo, fable, onyx, nova, shimmer
TTS_VOICE=alloy

# [OPTIONAL] API version (for Azure TTS)
TTS_API_VERSION=

# ==============================================================================
# WEB SEARCH CONFIGURATION
# ==============================================================================
# Optional: Enable web search capabilities for research and current information
#
# SUPPORTED PROVIDERS: perplexity, tavily, serper, exa

# [OPTIONAL] Web search provider (leave empty to disable search)
# Examples:
# perplexity - Fast AI-powered search
# tavily - Clean, structured search results
# serper - Google Search API
# exa - AI-powered web search
SEARCH_PROVIDER=perplexity

# [OPTIONAL] API key for your chosen search provider
# Get from: https://www.perplexity.ai/settings/api (Perplexity)
#          https://app.tavily.com/ (Tavily)
#          https://serper.dev/ (Serper)
#          https://exa.ai/ (Exa)
SEARCH_API_KEY=your-search-api-key-here

# ==============================================================================
# DEPLOYMENT & NETWORKING
# ==============================================================================

# [OPTIONAL] External API base URL for cloud/remote deployments
# Set this when deploying to a server accessible from the internet
# Examples:
#   Local development: leave empty
#   Cloud server: https://your-api-server.com
#   Railway/Docker: https://your-app.railway.app
#   Vercel: https://your-app.vercel.app
NEXT_PUBLIC_API_BASE_EXTERNAL=

# [OPTIONAL] Alternative API base URL (for advanced deployments)
NEXT_PUBLIC_API_BASE=

# ==============================================================================
# Local LLM Configuration (for Ollama, LM Studio, etc.)
# ==============================================================================

# [Optional] Template mode for chat formatting: auto, chatml, llama3, gemma, etc.
# 'auto' attempts to detect from model name, 'none' disables template formatting
LOCAL_LLM_TEMPLATE_MODE=auto

# [Optional] Custom chat template (Jinja2 format) - overrides template mode
# Example: "{{ bos_token }}{% for message in messages %}..."
LOCAL_LLM_CHAT_TEMPLATE=

# ==============================================================================
# Debug & Development
# ==============================================================================

# [Optional] Disable SSL verification (not recommended for production)
# Accepts: 'true', '1', 'yes' (case-insensitive)
DISABLE_SSL_VERIFY=false

# ==============================================================================
# DashScope Configuration (Qwen via OpenAI-compatible API)
# ==============================================================================
# Set the HTTP base URL for DashScope regional endpoints. Common options:
# - US:  https://dashscope-us.aliyuncs.com/compatible-mode/v1
# - SG:  https://dashscope-intl.aliyuncs.com/compatible-mode/v1
# - CN:  https://dashscope.aliyuncs.com/compatible-mode/v1
# Full chat completions endpoint example:
#   https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions

DASHSCOPE_API_KEY=
DASHSCOPE_HTTP_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1
