﻿﻿# ==============================================================================
# DeepTutor Environment Configuration
# =========================================================================
# Copy this file to `.env` and fill in the values.
# Required fields are marked with [Required], optional fields with [Optional].

# ==============================================================================
# Pytest Configuration
# ========================================================================

# [Optional] Set to 1 to run LLM network tests. Requires LLM keys to be populated.
# If not set or set to 0, LLM network tests will be skipped.

# SSL inspection must be disabled to avoid SSL/TLS errors during tests.
# Set to 'true' to disable SSL verification for LLM network tests.
# WARNING: Only use this for testing. Never disable SSL verification in production.
DISABLE_SSL_VERIFY=true
# [Optional] For VS Code users: To enable the Python extension to load environment variables from this .env file, set `python.terminal.useEnvFile` to 'True' in your VS Code settings (settings.json). This setting ensures the environment is correctly configured for network tests. Note that this setting is for the VS Code terminal, not for the Python interpreter itself. It is recommended to add this setting to your workspace settings (`.vscode/settings.json`) or user settings.
# If unable to set via IDE, you can manually load the .env file in your test setup code as follows (PowerShell example):
#
# $envContent = Get-Content .env -Raw
# foreach ($line in ($envContent -split "`n")) {
#     if ($line -match '^([^=]+)=(.*)# }

# If you need to test specific providers, use LLM_BINDING=<provider> with the integration tests.

# =====================================
# Server Ports
# ==============================================================================

# [Optional] Backend API server port
BACKEND_PORT=8001

# [Optional] Frontend server port
FRONTEND_PORT=3782

# ==============================================================================
# LLM Configuration (Large Language Model)
# ==============================================================================
# Primary LLM for all AI operations (chat, research, solve, etc.)

# [Required] Provider binding: openai, azure_openai, anthropic,
# deepseek, openrouter, groq, together, mistral
# ollama, lm_studio, vllm, llama_cpp
LLM_BINDING=openai

# [Required] Model name (e.g., gpt-4o, deepseek-chat, claude-3-5-sonnet)
LLM_MODEL=gpt-4o

# [Required] API key for the LLM provider
LLM_API_KEY=sk-xxx

# [Required] API endpoint URL
LLM_HOST=https://api.openai.com/v1

# [Optional] API version (required for Azure OpenAI)
LLM_API_VERSION=

# ==============================================================================
# Embedding Configuration
# ==============================================================================
# Embedding model for RAG (Retrieval-Augmented Generation)

# [Required] Provider: openai, azure_openai, jina,
# cohere, huggingface, google, ollama, lm_studio
EMBEDDING_BINDING=openai

# [Required] Model name
EMBEDDING_MODEL=text-embedding-3-small

# [Required] API key
EMBEDDING_API_KEY=sk-xxx

# [Required] API endpoint URL
EMBEDDING_HOST=https://api.openai.com/v1

# [Required] Vector dimensions (must match model output)
EMBEDDING_DIMENSION=3072

# [Optional] API version (for Azure OpenAI)
EMBEDDING_API_VERSION=

# ==============================================================================
# TTS Configuration (Text-to-Speech)
# ==============================================================================
# Optional: Enable audio narration features

# [Optional] Provider: openai, azure_openai
TTS_BINDING=openai

# [Optional] TTS model name
TTS_MODEL=tts-1

# [Optional] API key (can be same as LLM_API_KEY for OpenAI)
TTS_API_KEY=sk-xxx

# [Optional] API endpoint URL
TTS_URL=https://api.openai.com/v1

# [Optional] Voice: alloy, echo, fable, onyx, nova, shimmer
TTS_VOICE=alloy

# [Optional] API version (for Azure OpenAI)
TTS_BINDING_API_VERSION=

# ==============================================================================
# Search Configuration (Web Search)
# ==============================================================================
# Optional: Enable web search capabilities

# [Optional] Provider: perplexity, tavily, serper, jina, exa
SEARCH_PROVIDER=perplexity

# [Optional] API key for your chosen search provider
SEARCH_API_KEY=pplx-xxx

# ==============================================================================
# Cloud Deployment Configuration
# ==============================================================================
# Required when deploying to cloud/remote servers

# [Optional] External API base URL for cloud deployment
# Set this to your server's public URL when deploying remotely
# Example: https://your-server.com:8001 or https://api.yourdomain.com
NEXT_PUBLIC_API_BASE_EXTERNAL=

# [Optional] Direct API base URL (alternative to above)
NEXT_PUBLIC_API_BASE=

# ==============================================================================
# Local LLM Configuration (for Ollama, LM Studio, etc.)
# ==============================================================================

# [Optional] Template mode for chat formatting: auto, chatml, llama3, gemma, etc.
# 'auto' attempts to detect from model name, 'none' disables template formatting
LOCAL_LLM_TEMPLATE_MODE=auto

# [Optional] Custom chat template (Jinja2 format) - overrides template mode
# Example: "{{ bos_token }}{% for message in messages %}..."
LOCAL_LLM_CHAT_TEMPLATE=

# ==============================================================================
# Debug & Development
# ==============================================================================

# [Optional] Disable SSL verification (not recommended for production)
# Accepts: 'true', '1', 'yes' (case-insensitive)
DISABLE_SSL_VERIFY=false

# ==============================================================================
# DashScope Configuration (Qwen via OpenAI-compatible API)
# ==============================================================================
# Set the HTTP base URL for DashScope regional endpoints. Common options:
# - US:  https://dashscope-us.aliyuncs.com/compatible-mode/v1
# - SG:  https://dashscope-intl.aliyuncs.com/compatible-mode/v1
# - CN:  https://dashscope.aliyuncs.com/compatible-mode/v1
# Full chat completions endpoint example:
#   https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions

DASHSCOPE_API_KEY=
DASHSCOPE_HTTP_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1
) {
#         $key = $matches[1].Trim()
#         $value = $matches[2].Trim()
#         [Environment]::SetEnvironmentVariable($key, $value, "Process")
#     }
# }# }

# If you need to test specific providers, use LLM_BINDING=<provider> with the integration tests.

# =====================================
# Server Ports
# ==============================================================================

# [Optional] Backend API server port
BACKEND_PORT=8001

# [Optional] Frontend server port
FRONTEND_PORT=3782

# ==============================================================================
# LLM Configuration (Large Language Model)
# ==============================================================================
# Primary LLM for all AI operations (chat, research, solve, etc.)

# [Required] Provider binding: openai, azure_openai, anthropic,
# deepseek, openrouter, groq, together, mistral
# ollama, lm_studio, vllm, llama_cpp
LLM_BINDING=openai

# [Required] Model name (e.g., gpt-4o, deepseek-chat, claude-3-5-sonnet)
LLM_MODEL=gpt-4o

# [Required] API key for the LLM provider
LLM_API_KEY=sk-xxx

# [Required] API endpoint URL
LLM_HOST=https://api.openai.com/v1

# [Optional] API version (required for Azure OpenAI)
LLM_API_VERSION=

# ==============================================================================
# Embedding Configuration
# ==============================================================================
# Embedding model for RAG (Retrieval-Augmented Generation)

# [Required] Provider: openai, azure_openai, jina,
# cohere, huggingface, google, ollama, lm_studio
EMBEDDING_BINDING=openai

# [Required] Model name
EMBEDDING_MODEL=text-embedding-3-small

# [Required] API key
EMBEDDING_API_KEY=sk-xxx

# [Required] API endpoint URL
EMBEDDING_HOST=https://api.openai.com/v1

# [Required] Vector dimensions (must match model output)
EMBEDDING_DIMENSION=3072

# [Optional] API version (for Azure OpenAI)
EMBEDDING_API_VERSION=

# ==============================================================================
# TTS Configuration (Text-to-Speech)
# ==============================================================================
# Optional: Enable audio narration features

# [Optional] Provider: openai, azure_openai
TTS_BINDING=openai

# [Optional] TTS model name
TTS_MODEL=tts-1

# [Optional] API key (can be same as LLM_API_KEY for OpenAI)
TTS_API_KEY=sk-xxx

# [Optional] API endpoint URL
TTS_URL=https://api.openai.com/v1

# [Optional] Voice: alloy, echo, fable, onyx, nova, shimmer
TTS_VOICE=alloy

# [Optional] API version (for Azure OpenAI)
TTS_BINDING_API_VERSION=

# ==============================================================================
# Search Configuration (Web Search)
# ==============================================================================
# Optional: Enable web search capabilities

# [Optional] Provider: perplexity, tavily, serper, jina, exa
SEARCH_PROVIDER=perplexity

# [Optional] API key for your chosen search provider
SEARCH_API_KEY=pplx-xxx

# ==============================================================================
# Cloud Deployment Configuration
# ==============================================================================
# Required when deploying to cloud/remote servers

# [Optional] External API base URL for cloud deployment
# Set this to your server's public URL when deploying remotely
# Example: https://your-server.com:8001 or https://api.yourdomain.com
NEXT_PUBLIC_API_BASE_EXTERNAL=

# [Optional] Direct API base URL (alternative to above)
NEXT_PUBLIC_API_BASE=

# ==============================================================================
# Local LLM Configuration (for Ollama, LM Studio, etc.)
# ==============================================================================

# [Optional] Template mode for chat formatting: auto, chatml, llama3, gemma, etc.
# 'auto' attempts to detect from model name, 'none' disables template formatting
LOCAL_LLM_TEMPLATE_MODE=auto

# [Optional] Custom chat template (Jinja2 format) - overrides template mode
# Example: "{{ bos_token }}{% for message in messages %}..."
LOCAL_LLM_CHAT_TEMPLATE=

# ==============================================================================
# Debug & Development
# ==============================================================================

# [Optional] Disable SSL verification (not recommended for production)
# Accepts: 'true', '1', 'yes' (case-insensitive)
DISABLE_SSL_VERIFY=false

# ==============================================================================
# DashScope Configuration (Qwen via OpenAI-compatible API)
# ==============================================================================
# Set the HTTP base URL for DashScope regional endpoints. Common options:
# - US:  https://dashscope-us.aliyuncs.com/compatible-mode/v1
# - SG:  https://dashscope-intl.aliyuncs.com/compatible-mode/v1
# - CN:  https://dashscope.aliyuncs.com/compatible-mode/v1
# Full chat completions endpoint example:
#   https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions

DASHSCOPE_API_KEY=
DASHSCOPE_HTTP_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1
