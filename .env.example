# ==============================================================================
# DeepTutor Environment Configuration
# ==============================================================================
# Copy this file to `.env` and customize the values for your setup.
#
# Required fields are marked with [REQUIRED], optional fields with [OPTIONAL].
# Remove the # from the beginning of lines you want to use.
#
# For detailed setup instructions, see: docs/guide/configuration.md

# ==============================================================================
# SERVER CONFIGURATION
# ==============================================================================

# [OPTIONAL] Backend API server port (default: 8001)
# Change if port 8001 is already in use on your system
BACKEND_PORT=8001

# [OPTIONAL] Frontend server port (default: 3782)
# Change if port 3782 is already in use on your system
FRONTEND_PORT=3782

# ==============================================================================
# LLM CONFIGURATION (Large Language Model)
# ==============================================================================
# Primary LLM for all AI operations (chat, research, solve, code generation, etc.)
#
# SUPPORTED PROVIDERS:
# Cloud APIs: openai, anthropic, deepseek, groq, mistral, openrouter, together
# Local: ollama, lm_studio, vllm, llama_cpp
# Azure: azure_openai

# [REQUIRED] LLM provider binding
# Examples:
#   openai     - OpenAI API (GPT models)
#   anthropic  - Anthropic API (Claude models)
#   deepseek   - DeepSeek API (deepseek-chat, deepseek-reasoner)
#   groq       - Groq API (fast inference)
#   mistral    - Mistral AI API
#   openrouter - OpenRouter (access to many models)
#   together   - Together AI API
#   ollama     - Local Ollama server
#   lm_studio  - Local LM Studio
#   vllm       - Local vLLM server
#   llama_cpp  - Local llama.cpp server
#   azure_openai - Azure OpenAI service
LLM_BINDING=openai

# [REQUIRED] Model name - depends on your chosen provider
# Examples by provider:
# OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-5-sonnet-20241022, claude-3-haiku-20240307
# DeepSeek: deepseek-chat, deepseek-reasoner
# Groq: llama-3.2-90b-text-preview, mixtral-8x7b-32768
# Ollama: llama3.2:3b, codellama:7b, mistral:7b
# LM Studio: any model loaded in LM Studio
LLM_MODEL=gpt-4o

# [REQUIRED] API key for your LLM provider
# Get from: https://platform.openai.com/api-keys (OpenAI)
#          https://console.anthropic.com/ (Anthropic)
#          https://platform.deepseek.com/ (DeepSeek)
#          https://console.groq.com/ (Groq)
#          Not needed for local providers (ollama, lm_studio, etc.)
LLM_API_KEY=sk-your-openai-api-key-here

# [REQUIRED] API endpoint URL
# Examples:
# OpenAI: https://api.openai.com/v1
# Anthropic: https://api.anthropic.com/v1
# DeepSeek: https://api.deepseek.com
# Groq: https://api.groq.com/openai/v1
# Ollama: http://localhost:11434/v1
# LM Studio: http://localhost:1234/v1
# vLLM: http://localhost:8000/v1
# llama.cpp: http://localhost:8080/v1
# Azure: https://your-resource.openai.azure.com/
LLM_HOST=https://api.openai.com/v1

# [OPTIONAL] API version (required for Azure OpenAI)
# Examples: 2024-02-15-preview, 2023-12-01-preview
# Leave empty for non-Azure providers
LLM_API_VERSION=

# ==============================================================================
# LLM RETRY CONFIGURATION
# ==============================================================================
# Configure automatic retry behavior for LLM API calls

# [OPTIONAL] Maximum number of retry attempts (default: 3)
# Higher values increase reliability but may slow down responses
LLM_RETRY__MAX_RETRIES=3

# [OPTIONAL] Base delay between retries in seconds (default: 1.0)
# Used for exponential backoff calculations
LLM_RETRY__BASE_DELAY=1.0

# [OPTIONAL] Use exponential backoff for retries (default: true)
# When true: delays increase as 1s, 2s, 4s, 8s, etc.
# When false: fixed delay between all retries
LLM_RETRY__EXPONENTIAL_BACKOFF=true

# ==============================================================================
# EMBEDDING CONFIGURATION
# ==============================================================================
# Vector embeddings for RAG (Retrieval-Augmented Generation)
# Used to create searchable knowledge bases from documents
#
# SUPPORTED PROVIDERS: openai, azure_openai, cohere, jina, huggingface

# [REQUIRED] Embedding provider
# Examples: openai, azure_openai, cohere, jina
EMBEDDING_BINDING=openai

# [REQUIRED] Embedding model name
# Examples:
# OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# Cohere: embed-english-v3.0, embed-multilingual-v3.0
# Jina: jina-embeddings-v2-base-en, jina-embeddings-v2-small-en
EMBEDDING_MODEL=text-embedding-3-small

# [REQUIRED] API key for embedding provider
# Can be the same as LLM_API_KEY for OpenAI
EMBEDDING_API_KEY=sk-your-openai-api-key-here

# [REQUIRED] API endpoint URL for embeddings
# Usually the same as your LLM provider's base URL
EMBEDDING_HOST=https://api.openai.com/v1

# [REQUIRED] Vector dimensions (must match your chosen model)
# Examples:
# text-embedding-3-small: 1536
# text-embedding-3-large: 3072
# text-embedding-ada-002: 1536
EMBEDDING_DIMENSION=1536

# [OPTIONAL] API version (for Azure OpenAI embeddings)
EMBEDDING_API_VERSION=

# ==============================================================================
# TTS CONFIGURATION (Text-to-Speech)
# ==============================================================================
# Optional: Enable audio narration and voice features
#
# SUPPORTED PROVIDERS: openai, azure_openai

# [OPTIONAL] TTS provider (leave empty to disable TTS)
TTS_BINDING=openai

# [OPTIONAL] TTS model name
# OpenAI: tts-1, tts-1-hd
TTS_MODEL=tts-1

# [OPTIONAL] API key for TTS (can reuse LLM_API_KEY for OpenAI)
TTS_API_KEY=sk-your-openai-api-key-here

# [OPTIONAL] TTS API endpoint URL
TTS_HOST=https://api.openai.com/v1

# [OPTIONAL] Voice selection
# OpenAI voices: alloy, echo, fable, onyx, nova, shimmer
TTS_VOICE=alloy

# [OPTIONAL] API version (for Azure TTS)
TTS_API_VERSION=

# ==============================================================================
# WEB SEARCH CONFIGURATION
# ==============================================================================
# Optional: Enable web search capabilities for research and current information
#
# SUPPORTED PROVIDERS: perplexity, tavily, serper, exa

# [OPTIONAL] Web search provider (leave empty to disable search)
# Examples:
# perplexity - Fast AI-powered search
# tavily - Clean, structured search results
# serper - Google Search API
# exa - AI-powered web search
SEARCH_PROVIDER=perplexity

# [OPTIONAL] API key for your chosen search provider
# Get from: https://www.perplexity.ai/settings/api (Perplexity)
#          https://app.tavily.com/ (Tavily)
#          https://serper.dev/ (Serper)
#          https://exa.ai/ (Exa)
SEARCH_API_KEY=your-search-api-key-here

# ==============================================================================
# DEPLOYMENT & NETWORKING
# ==============================================================================

# [OPTIONAL] External API base URL for cloud/remote deployments
# Set this when deploying to a server accessible from the internet
# Examples:
#   Local development: leave empty
#   Cloud server: https://your-api-server.com
#   Railway/Docker: https://your-app.railway.app
#   Vercel: https://your-app.vercel.app
NEXT_PUBLIC_API_BASE_EXTERNAL=

# [OPTIONAL] Alternative API base URL (for advanced deployments)
NEXT_PUBLIC_API_BASE=

# ==============================================================================
# SECURITY & DEBUGGING
# ==============================================================================

# [OPTIONAL] Disable SSL certificate verification
# WARNING: Only set to true for local development/testing!
# NEVER use in production - this creates security vulnerabilities
DISABLE_SSL_VERIFY=false

# ==============================================================================
# LOCAL LLM SETUP EXAMPLES
# ==============================================================================
# Uncomment and modify the sections below for local LLM setups

# === OLLAMA SETUP ===
# LLM_BINDING=ollama
# LLM_MODEL=llama3.2:3b
# LLM_API_KEY=ollama
# LLM_HOST=http://localhost:11434/v1

# === LM STUDIO SETUP ===
# LLM_BINDING=lm_studio
# LLM_MODEL=your-loaded-model-name
# LLM_API_KEY=lm-studio
# LLM_HOST=http://localhost:1234/v1

# === VLLM SETUP ===
# LLM_BINDING=vllm
# LLM_MODEL=your-model-name
# LLM_API_KEY=vllm
# LLM_HOST=http://localhost:8000/v1

# === LLAMA.CPP SETUP ===
# LLM_BINDING=llama_cpp
# LLM_MODEL=your-model-name
# LLM_API_KEY=llama-cpp
# LLM_HOST=http://localhost:8080/v1

# ==============================================================================
# TROUBLESHOOTING TIPS
# ==============================================================================
#
# 1. API Key Issues:
#    - Ensure your API key starts with the correct prefix (sk- for OpenAI)
#    - Check your account has sufficient credits/quota
#    - Verify the key has the required permissions
#
# 2. Local LLM Issues:
#    - Ensure your local server is running and accessible
#    - Check the port numbers match your server configuration
#    - Verify the model is loaded in your local server
#
# 3. Connection Issues:
#    - Test with curl: curl -X POST http://localhost:11434/v1/chat/completions
#    - Check firewall settings if running in containers
#    - Verify SSL settings for cloud providers
#
# 4. Performance Issues:
#    - Lower retry counts for faster responses
#    - Use smaller models for quicker inference
#    - Check network latency to your provider
#
# For detailed troubleshooting, see: docs/guide/troubleshooting.md
